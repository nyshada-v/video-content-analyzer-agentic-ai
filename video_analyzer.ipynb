{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIFzA7zTbeFH"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install gradio\n",
        "!pip install openai\n",
        "!pip install chromadb\n",
        "!pip install moviepy\n",
        "!pip install opencv-python\n",
        "!pip install python-dotenv\n",
        "!pip install langchain\n",
        "!pip install langchain-openai\n",
        "!pip install tiktoken\n",
        "\n",
        "print(\"‚úÖ All libraries installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cONLGt3grvk0"
      },
      "outputs": [],
      "source": [
        "!pip install -U google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRpz8aIwpdlc",
        "outputId": "5362bd63-c1b3-4dd5-a59e-26ba40aacc90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ API keys loaded from secrets!\n"
          ]
        }
      ],
      "source": [
        "# Load API keys from Colab Secrets (persists across sessions!)\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "os.environ[\"ASSEMBLYAI_API_KEY\"] = userdata.get('ASSEMBLYAI_API_KEY')\n",
        "\n",
        "print(\"‚úÖ API keys loaded from secrets!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-P7sNBCprn5",
        "outputId": "5bde661c-bf00-4e57-c111-1368bc18a52d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/51.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m51.6/51.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h‚úÖ Gemini and AssemblyAI libraries installed!\n"
          ]
        }
      ],
      "source": [
        "# Install Gemini and AssemblyAI libraries\n",
        "!pip install -q google-generativeai\n",
        "!pip install -q assemblyai\n",
        "\n",
        "print(\"‚úÖ Gemini and AssemblyAI libraries installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "0LS-Hg53uzGL",
        "outputId": "fc0e3407-c5ba-4d90-efee-bff4b89b5856"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Setting up APIs...\n",
            "\n",
            "‚úÖ API keys loaded from Colab secrets\n",
            "‚úÖ Gemini API Test:\n",
            "Gemini API is working!\n",
            "\n",
            "\n",
            "‚úÖ AssemblyAI API: Connected successfully!\n",
            "\n",
            "üéâ Setup complete! Ready to build!\n"
          ]
        }
      ],
      "source": [
        "# Final working version for Google Colab - WITH COLAB SECRETS\n",
        "import google.generativeai as genai\n",
        "import assemblyai as aai\n",
        "import os\n",
        "from google.colab import userdata  # Add this import\n",
        "\n",
        "print(\"üîß Setting up APIs...\\n\")\n",
        "\n",
        "# Configure APIs with Colab Secrets\n",
        "try:\n",
        "    # Load from Colab secrets\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    ASSEMBLYAI_API_KEY = userdata.get('ASSEMBLYAI_API_KEY')\n",
        "\n",
        "    print(\"‚úÖ API keys loaded from Colab secrets\")\n",
        "\n",
        "    # Configure Gemini API\n",
        "    genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "    # Test Gemini API\n",
        "    try:\n",
        "        model = genai.GenerativeModel('models/gemini-2.0-flash')\n",
        "        response = model.generate_content(\"Say 'Gemini API is working!'\")\n",
        "        print(\"‚úÖ Gemini API Test:\")\n",
        "        print(response.text)\n",
        "        print()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Gemini Error: {e}\")\n",
        "        print(\"\\nTrying alternative model...\")\n",
        "        try:\n",
        "            model = genai.GenerativeModel('models/gemini-1.5-flash')\n",
        "            response = model.generate_content(\"Say 'Gemini API is working!'\")\n",
        "            print(\"‚úÖ Gemini API Test (with gemini-1.5-flash):\")\n",
        "            print(response.text)\n",
        "            print()\n",
        "        except Exception as e2:\n",
        "            print(f\"‚ùå Still error: {e2}\\n\")\n",
        "\n",
        "    # Test AssemblyAI API\n",
        "    try:\n",
        "        aai.settings.api_key = ASSEMBLYAI_API_KEY\n",
        "        transcriber = aai.Transcriber()\n",
        "        print(\"‚úÖ AssemblyAI API: Connected successfully!\")\n",
        "        print()\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå AssemblyAI Error: {e}\\n\")\n",
        "\n",
        "    print(\"üéâ Setup complete! Ready to build!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error loading API keys: {e}\")\n",
        "    print(\"\\nüí° Make sure you've set up Colab secrets:\")\n",
        "    print(\"1. Click the üîë key icon in left sidebar\")\n",
        "    print(\"2. Add two secrets:\")\n",
        "    print(\"   - Name: GOOGLE_API_KEY, Value: your_gemini_api_key\")\n",
        "    print(\"   - Name: ASSEMBLYAI_API_KEY, Value: your_assemblyai_api_key\")\n",
        "    print(\"3. Restart runtime and run this cell again\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XFCAOdGt7ONd"
      },
      "outputs": [],
      "source": [
        "#@title Upload video here\n",
        "# Create a sample upload area\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "print(\"üìπ Upload a test video file (MP4, MOV, AVI, etc.)\")\n",
        "print(\"Recommended: A short video (1-3 minutes) for testing\\n\")\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the uploaded filename\n",
        "video_filename = list(uploaded.keys())[0]\n",
        "print(f\"\\n‚úÖ Video uploaded: {video_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFP_zMq2huV7",
        "outputId": "eb64f8e5-e638-44e1-e615-4143523ab20a"
      },
      "outputs": [],
      "source": [
        "!pip install assemblyai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07UMr3mh7ymG",
        "outputId": "6d4ffdae-0316-40bd-ca57-98db3832fc70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ AssemblyAI API key loaded from Colab secrets\n",
            "‚úÖ Transcription Agent initialized\n",
            "\n",
            "üéØ Transcription Agent ready to use!\n"
          ]
        }
      ],
      "source": [
        "#@title Transcript Agent\n",
        "# AGENT 1: Transcription Agent\n",
        "import assemblyai as aai\n",
        "from moviepy.editor import VideoFileClip\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "class TranscriptionAgent:\n",
        "    def __init__(self, assemblyai_api_key):\n",
        "        \"\"\"Initialize the Transcription Agent\"\"\"\n",
        "        aai.settings.api_key = assemblyai_api_key\n",
        "        self.transcriber = aai.Transcriber()\n",
        "        print(\"‚úÖ Transcription Agent initialized\")\n",
        "\n",
        "    def extract_audio(self, video_path):\n",
        "        \"\"\"Extract audio from video file\"\"\"\n",
        "        try:\n",
        "            print(f\"üé¨ Extracting audio from: {video_path}\")\n",
        "            video = VideoFileClip(video_path)\n",
        "            audio_path = \"extracted_audio.mp3\"\n",
        "            video.audio.write_audiofile(audio_path, verbose=False, logger=None)\n",
        "            video.close()\n",
        "            print(f\"‚úÖ Audio extracted: {audio_path}\")\n",
        "            return audio_path\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Audio extraction error: {e}\")\n",
        "            return None\n",
        "\n",
        "    def transcribe_audio(self, audio_path):\n",
        "        \"\"\"Transcribe audio file using AssemblyAI\"\"\"\n",
        "        try:\n",
        "            print(f\"üé§ Transcribing audio... (this may take a minute)\")\n",
        "\n",
        "            config = aai.TranscriptionConfig(\n",
        "                speech_model=aai.SpeechModel.best,\n",
        "                punctuate=True,\n",
        "                format_text=True\n",
        "            )\n",
        "\n",
        "            transcript = self.transcriber.transcribe(audio_path, config=config)\n",
        "\n",
        "            if transcript.status == aai.TranscriptStatus.error:\n",
        "                print(f\"‚ùå Transcription error: {transcript.error}\")\n",
        "                return None\n",
        "\n",
        "            print(\"‚úÖ Transcription complete!\")\n",
        "            return {\n",
        "                'text': transcript.text,\n",
        "                'words': transcript.words,\n",
        "                'duration': transcript.audio_duration\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Transcription error: {e}\")\n",
        "            return None\n",
        "\n",
        "    def process_video(self, video_path):\n",
        "        \"\"\"Complete workflow: extract audio + transcribe\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"ü§ñ TRANSCRIPTION AGENT STARTED\")\n",
        "        print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "        # Step 1: Extract audio\n",
        "        audio_path = self.extract_audio(video_path)\n",
        "        if not audio_path:\n",
        "            return None\n",
        "\n",
        "        # Step 2: Transcribe\n",
        "        result = self.transcribe_audio(audio_path)\n",
        "\n",
        "        if result:\n",
        "            print(f\"\\nüìä Transcription Stats:\")\n",
        "            print(f\"   Duration: {result['duration']/1000:.1f} seconds\")\n",
        "            print(f\"   Words: {len(result['words'])}\")\n",
        "            print(f\"   Characters: {len(result['text'])}\")\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "        return result\n",
        "\n",
        "# Initialize the agent - FIXED VERSION\n",
        "try:\n",
        "    # Method 1: Using Colab userdata (recommended)\n",
        "    ASSEMBLYAI_API_KEY = userdata.get('ASSEMBLYAI_API_KEY')\n",
        "    print(\"‚úÖ AssemblyAI API key loaded from Colab secrets\")\n",
        "except Exception as e:\n",
        "    try:\n",
        "        # Method 2: Traditional environment variable\n",
        "        ASSEMBLYAI_API_KEY = os.environ[\"ASSEMBLYAI_API_KEY\"]\n",
        "        print(\"‚úÖ AssemblyAI API key loaded from environment variable\")\n",
        "    except:\n",
        "        print(\"‚ùå Could not load AssemblyAI API key. Please check your Colab secrets.\")\n",
        "        ASSEMBLYAI_API_KEY = None\n",
        "\n",
        "if ASSEMBLYAI_API_KEY:\n",
        "    transcription_agent = TranscriptionAgent(ASSEMBLYAI_API_KEY)\n",
        "    print(\"\\nüéØ Transcription Agent ready to use!\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  Please set up your ASSEMBLYAI_API_KEY in Colab secrets first!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "00SVmsKj-Q6Q"
      },
      "outputs": [],
      "source": [
        "#@title write the transcript to file\n",
        "# Test the Transcription Agent\n",
        "result = transcription_agent.process_video(video_filename)\n",
        "\n",
        "if result:\n",
        "    print(\"\\nüìù TRANSCRIPT PREVIEW (first 500 characters):\")\n",
        "    print(\"-\" * 50)\n",
        "    print(result['text'][:500] + \"...\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Save full transcript\n",
        "    with open('transcript.txt', 'w') as f:\n",
        "        f.write(result['text'])\n",
        "    print(\"\\nüíæ Full transcript saved to: transcript.txt\")\n",
        "else:\n",
        "    print(\"\\n‚ùå Transcription failed. Please check the error messages above.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0FsLSBFkBkT",
        "outputId": "fb1d987d-83ec-4573-907e-e32a2f49b44f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Google API key loaded from Colab secrets\n",
            "‚úÖ Summary Agent initialized\n",
            "\n",
            "üéØ Summary Agent ready to use!\n"
          ]
        }
      ],
      "source": [
        "#@title AGENT 2: Summary Agent\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "class SummaryAgent:\n",
        "    def __init__(self, gemini_api_key):\n",
        "        \"\"\"Initialize the Summary Agent with Gemini\"\"\"\n",
        "        genai.configure(api_key=gemini_api_key)\n",
        "        self.model = genai.GenerativeModel('models/gemini-2.0-flash')\n",
        "        print(\"‚úÖ Summary Agent initialized\")\n",
        "\n",
        "    def generate_summary(self, transcript_text):\n",
        "        \"\"\"Generate a comprehensive summary of the transcript.\"\"\"\n",
        "        try:\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(\"ü§ñ SUMMARY AGENT STARTED\")\n",
        "            print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "            print(\"üìù Analyzing transcript and generating summary...\")\n",
        "\n",
        "            prompt = f\"\"\"\n",
        "            Analyze the following video transcript and provide:\n",
        "\n",
        "            1. **Main Topic**: What is this video about? (1-2 sentences)\n",
        "            2. **Key Points**: List the 5-7 most important points discussed (bullet points)\n",
        "            3. **Brief Summary**: A concise 2-3 paragraph summary of the entire content. can be less if video is short\n",
        "            4. **Key Takeaways**: 3-5 actionable insights or conclusions\n",
        "\n",
        "            Transcript:\n",
        "            {transcript_text}\n",
        "\n",
        "            Format your response clearly with headers for each section. and use bold wherever required.\n",
        "            \"\"\"\n",
        "\n",
        "            response = self.model.generate_content(prompt)\n",
        "\n",
        "            print(\"‚úÖ Summary generated successfully!\\n\")\n",
        "            print(\"=\"*50)\n",
        "\n",
        "            return response.text\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Summary generation error: {e}\")\n",
        "            return None\n",
        "\n",
        "    def generate_chapter_markers(self, transcript_text):\n",
        "        \"\"\"Generate chapter markers/timestamps for the video\"\"\"\n",
        "        try:\n",
        "            print(\"\\nüìå Generating chapter markers...\")\n",
        "\n",
        "            prompt = f\"\"\"\n",
        "            Based on this video transcript, identify the main topics/sections discussed.\n",
        "            Use bold to highlight info.\n",
        "            Create chapter markers with:\n",
        "            - Chapter title (concise, descriptive)\n",
        "            - Brief description (1 sentence)\n",
        "\n",
        "            Since we don't have exact timestamps, suggest logical breakpoints.\n",
        "            Create 3-5 chapters depending on content.\n",
        "\n",
        "            Transcript:\n",
        "            {transcript_text}\n",
        "\n",
        "            Format as:\n",
        "            Chapter 1: [Title]\n",
        "            - Description: [1 sentence]\n",
        "\n",
        "            Chapter 2: [Title]\n",
        "            - Description: [1 sentence]\n",
        "\n",
        "            (etc.)\n",
        "            \"\"\"\n",
        "\n",
        "            response = self.model.generate_content(prompt)\n",
        "\n",
        "            print(\"‚úÖ Chapter markers generated!\\n\")\n",
        "\n",
        "            return response.text\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Chapter marker error: {e}\")\n",
        "            return None\n",
        "\n",
        "    def process_transcript(self, transcript_text):\n",
        "        \"\"\"Complete workflow: generate summary + chapter markers\"\"\"\n",
        "        # Generate summary\n",
        "        summary = self.generate_summary(transcript_text)\n",
        "\n",
        "        # Generate chapters\n",
        "        chapters = self.generate_chapter_markers(transcript_text)\n",
        "\n",
        "        return {\n",
        "            'summary': summary,\n",
        "            'chapters': chapters\n",
        "        }\n",
        "\n",
        "# Initialize the Summary Agent - FIXED VERSION\n",
        "try:\n",
        "    # Method 1: Using Colab userdata\n",
        "    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "    print(\"‚úÖ Google API key loaded from Colab secrets\")\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Could not load Google API key. Please check your Colab secrets.\")\n",
        "    GOOGLE_API_KEY = None\n",
        "\n",
        "if GOOGLE_API_KEY:\n",
        "    summary_agent = SummaryAgent(GOOGLE_API_KEY)\n",
        "    print(\"\\nüéØ Summary Agent ready to use!\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  Please set up your GOOGLE_API_KEY in Colab secrets first!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LoxrxkdVmdnr"
      },
      "outputs": [],
      "source": [
        "#@title Test the Summary Agent with the transcript\n",
        "with open('transcript.txt', 'r') as f:\n",
        "    transcript = f.read()\n",
        "\n",
        "# Generate summary and chapters\n",
        "results = summary_agent.process_transcript(transcript)\n",
        "\n",
        "# Display results\n",
        "if results['summary']:\n",
        "    print(\"\\nüìä SUMMARY:\")\n",
        "    print(\"=\"*60)\n",
        "    print(results['summary'])\n",
        "    print(\"=\"*60)\n",
        "\n",
        "if results['chapters']:\n",
        "    print(\"\\n\\nüìë CHAPTER MARKERS:\")\n",
        "    print(\"=\"*60)\n",
        "    print(results['chapters'])\n",
        "    print(\"=\"*60)\n",
        "\n",
        "# Save to file\n",
        "with open('summary_output.txt', 'w') as f:\n",
        "    f.write(\"VIDEO SUMMARY\\n\")\n",
        "    f.write(\"=\"*60 + \"\\n\\n\")\n",
        "    f.write(results['summary'])\n",
        "    f.write(\"\\n\\n\" + \"=\"*60 + \"\\n\\n\")\n",
        "    f.write(\"CHAPTER MARKERS\\n\")\n",
        "    f.write(\"=\"*60 + \"\\n\\n\")\n",
        "    f.write(results['chapters'])\n",
        "\n",
        "print(\"\\n\\nüíæ Summary saved to: summary_output.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "23GeaPD696E9",
        "outputId": "a5daee28-d5fc-4790-b57b-372827f0da57"
      },
      "outputs": [],
      "source": [
        "# Install ChromaDB with proper dependency handling\n",
        "# !pip install --upgrade pip setuptools wheel\n",
        "!pip install jedi>=0.16  # Fix the missing dependency\n",
        "!pip install chromadb\n",
        "\n",
        "print(\"‚úÖ ChromaDB installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXvps7T--kjv",
        "outputId": "c1143d03-147e-42c7-a87c-5a5b8677b2f3"
      },
      "outputs": [],
      "source": [
        "#@title Embedding Agent\n",
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "import google.generativeai as genai\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "from typing import List, Dict, Any, Optional\n",
        "import logging\n",
        "\n",
        "class EnhancedEmbeddingAgent:\n",
        "    def __init__(self, embedding_model: str = \"sentence_transformer\", gemini_api_key: Optional[str] = None):\n",
        "        \"\"\"\n",
        "        Initialize Embedding Agent with multiple embedding options\n",
        "\n",
        "        Args:\n",
        "            embedding_model: \"sentence_transformer\" (free), \"gemini\" (paid), or \"openai\" (paid)\n",
        "            gemini_api_key: Optional API key for Gemini (only needed if using Gemini)\n",
        "        \"\"\"\n",
        "        self.embedding_model = embedding_model\n",
        "        self.chroma_client = chromadb.Client()\n",
        "\n",
        "        # Clean up existing collection\n",
        "        try:\n",
        "            self.chroma_client.delete_collection(\"video_transcripts\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Initialize embedding function based on choice\n",
        "        if embedding_model == \"sentence_transformer\":\n",
        "            print(\"üÜì Using free SentenceTransformer embeddings (all-MiniLM-L6-v2)\")\n",
        "            self.embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
        "                model_name=\"all-MiniLM-L6-v2\"\n",
        "            )\n",
        "        elif embedding_model == \"gemini\" and gemini_api_key:\n",
        "            print(\"üîë Using Gemini embeddings (paid)\")\n",
        "            genai.configure(api_key=gemini_api_key)\n",
        "            self.embedding_function = self._gemini_embedding_function\n",
        "        elif embedding_model == \"openai\":\n",
        "            print(\"üîë Using OpenAI embeddings (paid)\")\n",
        "            raise NotImplementedError(\"OpenAI embeddings not implemented in this example\")\n",
        "        else:\n",
        "            print(\"üÜì Defaulting to free SentenceTransformer embeddings\")\n",
        "            self.embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
        "                model_name=\"all-MiniLM-L6-v2\"\n",
        "            )\n",
        "\n",
        "        # Create collection\n",
        "        self.collection = self.chroma_client.create_collection(\n",
        "            name=\"video_transcripts\",\n",
        "            embedding_function=self.embedding_function\n",
        "        )\n",
        "\n",
        "        # Initialize SentenceTransformer for manual embeddings if needed\n",
        "        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "        print(f\"‚úÖ Embedding Agent initialized with {embedding_model}\")\n",
        "        print(f\"‚úÖ Vector database created: {self.collection.name}\")\n",
        "\n",
        "    def _gemini_embedding_function(self, texts: List[str]) -> List[List[float]]:\n",
        "        \"\"\"Custom embedding function for Gemini\"\"\"\n",
        "        try:\n",
        "            embeddings = []\n",
        "            for text in texts:\n",
        "                # Gemini embedding API call\n",
        "                result = genai.embed_content(\n",
        "                    model=\"models/embedding-001\",\n",
        "                    content=text,\n",
        "                    task_type=\"retrieval_document\"\n",
        "                )\n",
        "                embeddings.append(result['embedding'])\n",
        "            return embeddings\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Gemini embedding failed: {e}\")\n",
        "            # Fallback to SentenceTransformer\n",
        "            print(\"üîÑ Falling back to SentenceTransformer embeddings\")\n",
        "            return self.sentence_model.encode(texts).tolist()\n",
        "\n",
        "    def chunk_transcript(self, transcript_text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
        "        \"\"\"Split transcript into overlapping chunks with smart paragraph boundaries\"\"\"\n",
        "        words = transcript_text.split()\n",
        "        chunks = []\n",
        "\n",
        "        # If transcript is short, return as single chunk\n",
        "        if len(words) <= chunk_size:\n",
        "            return [transcript_text]\n",
        "\n",
        "        # Smart chunking with sentence/paragraph awareness\n",
        "        sentences = transcript_text.split('. ')\n",
        "        current_chunk = []\n",
        "        current_length = 0\n",
        "\n",
        "        for sentence in sentences:\n",
        "            sentence_words = sentence.split()\n",
        "            sentence_length = len(sentence_words)\n",
        "\n",
        "            # If adding this sentence exceeds chunk size, save current chunk\n",
        "            if current_length + sentence_length > chunk_size and current_chunk:\n",
        "                chunks.append(' '.join(current_chunk))\n",
        "                # Start new chunk with overlap\n",
        "                overlap_words = current_chunk[-overlap:] if len(current_chunk) > overlap else current_chunk\n",
        "                current_chunk = overlap_words\n",
        "                current_length = len(overlap_words)\n",
        "\n",
        "            current_chunk.extend(sentence_words)\n",
        "            current_length += sentence_length\n",
        "\n",
        "        # Add final chunk\n",
        "        if current_chunk:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "\n",
        "        print(f\"‚úÇÔ∏è  Created {len(chunks)} chunks from transcript\")\n",
        "        return chunks\n",
        "\n",
        "    def store_embeddings(self, transcript_text: str, metadata: Dict[str, Any] = None) -> Dict[str, Any]:\n",
        "        \"\"\"Chunk transcript and store embeddings in vector DB\"\"\"\n",
        "        try:\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(\"ü§ñ ENHANCED EMBEDDING AGENT STARTED\")\n",
        "            print(\"=\"*50)\n",
        "\n",
        "            print(\"‚úÇÔ∏è  Chunking transcript with smart boundaries...\")\n",
        "            chunks = self.chunk_transcript(transcript_text)\n",
        "\n",
        "            if not chunks:\n",
        "                print(\"‚ùå No chunks created from transcript\")\n",
        "                return None\n",
        "\n",
        "            print(f\"‚úÖ Created {len(chunks)} chunks\")\n",
        "            print(f\"üìä Chunk sizes: {[len(chunk.split()) for chunk in chunks]} words\")\n",
        "\n",
        "            print(\"\\nüß† Generating embeddings...\")\n",
        "\n",
        "            # Prepare metadata for each chunk\n",
        "            base_metadata = metadata or {}\n",
        "            metadatas = []\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                chunk_metadata = base_metadata.copy()\n",
        "                chunk_metadata.update({\n",
        "                    \"chunk_index\": i,\n",
        "                    \"chunk_length\": len(chunk),\n",
        "                    \"word_count\": len(chunk.split()),\n",
        "                    \"embedding_model\": self.embedding_model\n",
        "                })\n",
        "                metadatas.append(chunk_metadata)\n",
        "\n",
        "            # Add to ChromaDB collection\n",
        "            ids = [f\"chunk_{i}_{hash(chunk) % 10000}\" for i, chunk in enumerate(chunks)]\n",
        "\n",
        "            self.collection.add(\n",
        "                documents=chunks,\n",
        "                ids=ids,\n",
        "                metadatas=metadatas\n",
        "            )\n",
        "\n",
        "            # Verify storage\n",
        "            collection_count = self.collection.count()\n",
        "            print(f\"‚úÖ Stored {collection_count} chunks in vector database\")\n",
        "\n",
        "            stats = {\n",
        "                'total_chunks': len(chunks),\n",
        "                'collection_name': self.collection.name,\n",
        "                'embedding_model': self.embedding_model,\n",
        "                'average_chunk_size': np.mean([len(chunk.split()) for chunk in chunks])\n",
        "            }\n",
        "\n",
        "            print(f\"üìä Embedding Stats: {stats}\")\n",
        "            print(\"=\"*50)\n",
        "\n",
        "            return stats\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Embedding storage error: {e}\")\n",
        "            logging.error(f\"Embedding storage failed: {e}\")\n",
        "            return None\n",
        "\n",
        "    def semantic_search(self, query: str, n_results: int = 3, filter_metadata: Optional[Dict] = None) -> Dict[str, Any]:\n",
        "        \"\"\"Enhanced semantic search with filtering\"\"\"\n",
        "        try:\n",
        "            print(f\"üîç Performing semantic search: '{query}'\")\n",
        "\n",
        "            results = self.collection.query(\n",
        "                query_texts=[query],\n",
        "                n_results=n_results,\n",
        "                where=filter_metadata  # Optional metadata filtering\n",
        "            )\n",
        "\n",
        "            if results and results['documents']:\n",
        "                print(f\"‚úÖ Found {len(results['documents'][0])} relevant results\")\n",
        "                return {\n",
        "                    'documents': results['documents'][0],\n",
        "                    'metadatas': results['metadatas'][0],\n",
        "                    'distances': results['distances'][0],\n",
        "                    'query': query\n",
        "                }\n",
        "            else:\n",
        "                print(\"‚ùå No results found\")\n",
        "                return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Semantic search error: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_collection_stats(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get statistics about the vector database\"\"\"\n",
        "        try:\n",
        "            count = self.collection.count()\n",
        "            # Get sample of documents to analyze\n",
        "            sample_results = self.collection.get(limit=min(10, count))\n",
        "\n",
        "            stats = {\n",
        "                'total_documents': count,\n",
        "                'embedding_model': self.embedding_model,\n",
        "                'sample_chunk_sizes': [len(doc.split()) for doc in sample_results['documents']] if sample_results['documents'] else []\n",
        "            }\n",
        "\n",
        "            return stats\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error getting collection stats: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def clear_database(self):\n",
        "        \"\"\"Clear the vector database\"\"\"\n",
        "        try:\n",
        "            self.chroma_client.delete_collection(\"video_transcripts\")\n",
        "            self.collection = self.chroma_client.create_collection(\n",
        "                name=\"video_transcripts\",\n",
        "                embedding_function=self.embedding_function\n",
        "            )\n",
        "            print(\"‚úÖ Vector database cleared\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error clearing database: {e}\")\n",
        "\n",
        "# Initialize with free embeddings by default\n",
        "embedding_agent = EnhancedEmbeddingAgent(embedding_model=\"sentence_transformer\")\n",
        "print(\"\\nüéØ Enhanced Embedding Agent ready! (Using free embeddings)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2A-1YGHCaY0",
        "outputId": "0367bfab-8990-49ac-cf58-808936ec137f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ Testing Embedding Agent...\n",
            "‚ùå transcript.txt file not found. Please run the transcription agent first.\n"
          ]
        }
      ],
      "source": [
        "#@title Test the Embedding Agent\n",
        "print(\"üß™ Testing Embedding Agent...\")\n",
        "\n",
        "# Read the transcript file\n",
        "try:\n",
        "    with open('transcript.txt', 'r') as f:\n",
        "        transcript = f.read()\n",
        "    print(f\"üìñ Loaded transcript: {len(transcript)} characters\")\n",
        "\n",
        "    # Store embeddings\n",
        "    result = embedding_agent.store_embeddings(transcript)\n",
        "\n",
        "    if result:\n",
        "        print(f\"üìä Embedding Stats:\")\n",
        "        print(f\"   Total chunks: {result['total_chunks']}\")\n",
        "        print(f\"   Collection: {result['collection_name']}\")\n",
        "\n",
        "        # Test search functionality\n",
        "        print(\"\\nüîç Testing search functionality...\")\n",
        "        test_query = \"main topic of the video\"\n",
        "        search_results = embedding_agent.search_similar(test_query, n_results=2)\n",
        "\n",
        "        if search_results:\n",
        "            print(f\"‚úÖ Search test successful!\")\n",
        "            print(f\"   Query: '{test_query}'\")\n",
        "            print(f\"   Found {len(search_results['documents'][0])} results\")\n",
        "        else:\n",
        "            print(\"‚ùå Search test failed\")\n",
        "\n",
        "        print(f\"\\n‚úÖ Vector database is ready for Q&A!\")\n",
        "    else:\n",
        "        print(\"‚ùå Failed to store embeddings\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"‚ùå transcript.txt file not found. Please run the transcription agent first.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Testing error: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTeSkv9CCjBQ",
        "outputId": "07e05af7-e256-4ad1-d4c8-49b75082a81c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Q&A Agent initialized\n",
            "\n",
            "üéØ Q&A Agent ready to use!\n"
          ]
        }
      ],
      "source": [
        "#@title AGENT 4: Q&A Agent\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "class QAAgent:\n",
        "    def __init__(self, embedding_agent):\n",
        "        \"\"\"Initialize Q&A Agent with Gemini and access to vector DB\"\"\"\n",
        "        # Get API key from Colab secrets\n",
        "        gemini_api_key = userdata.get('GOOGLE_API_KEY')\n",
        "        genai.configure(api_key=gemini_api_key)\n",
        "        self.model = genai.GenerativeModel('models/gemini-2.0-flash')\n",
        "        self.embedding_agent = embedding_agent\n",
        "        print(\"‚úÖ Q&A Agent initialized\")\n",
        "\n",
        "    def answer_question(self, question):\n",
        "        \"\"\"Answer a question using RAG (Retrieval Augmented Generation)\"\"\"\n",
        "        try:\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(\"ü§ñ Q&A AGENT STARTED\")\n",
        "            print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "            print(f\"‚ùì Question: {question}\\n\")\n",
        "\n",
        "            # Step 1: Search vector DB for relevant context\n",
        "            print(\"üîç Searching vector database for relevant content...\")\n",
        "\n",
        "            # Use the semantic_search method that exists in your embedding agent\n",
        "            search_results = self.embedding_agent.semantic_search(question, n_results=3)\n",
        "\n",
        "            print(f\"üìä Search results type: {type(search_results)}\")\n",
        "\n",
        "            if not search_results or not search_results.get('documents'):\n",
        "                print(\"‚ùå No documents found in search results\")\n",
        "                print(f\"üîç Search results: {search_results}\")\n",
        "                return \"I couldn't find relevant information in the video to answer this question.\"\n",
        "\n",
        "            # Get relevant chunks - handle the response format\n",
        "            relevant_chunks = search_results['documents']\n",
        "            context = \"\\n\\n\".join(relevant_chunks)\n",
        "\n",
        "            print(f\"‚úÖ Found {len(relevant_chunks)} relevant sections\")\n",
        "            print(f\"üìù First chunk preview: {relevant_chunks[0][:200]}...\\n\")\n",
        "\n",
        "            # Step 2: Generate answer using context\n",
        "            print(\"üí≠ Generating answer using context...\")\n",
        "\n",
        "            prompt = f\"\"\"\n",
        "            Based on the following content from a video transcript, answer the user's question.\n",
        "\n",
        "            Context from video:\n",
        "            {context}\n",
        "\n",
        "            User's question: {question}\n",
        "\n",
        "            Instructions:\n",
        "            - Answer directly and concisely\n",
        "            - If the user is asking questions abount the video and not the content in the video, then use your knowledge and answer him. In this case, dont say Idk\n",
        "            - Use only information from the provided context to answer questions related to it.\n",
        "            - If the context doesn't contain what user is asking, say \"I cannot find this information in the video\"\n",
        "            - Be helpful and clear\n",
        "            - Interact with the user in a friendly way\n",
        "            - You can use emojis and images to make the user understand better\n",
        "\n",
        "            Answer:\n",
        "            \"\"\"\n",
        "\n",
        "            response = self.model.generate_content(prompt)\n",
        "            answer = response.text\n",
        "\n",
        "            print(\"‚úÖ Answer generated!\")\n",
        "            print(f\"ü§ñ Answer: {answer}\\n\")\n",
        "            print(\"=\"*50)\n",
        "\n",
        "            return {\n",
        "                'question': question,\n",
        "                'answer': answer,\n",
        "                'sources': relevant_chunks\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Q&A error: {e}\")\n",
        "            import traceback\n",
        "            print(f\"üîç Full traceback: {traceback.format_exc()}\")\n",
        "            return None\n",
        "\n",
        "qa_agent = QAAgent(embedding_agent)\n",
        "print(\"\\nüéØ Q&A Agent ready to use!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "id": "VLdyKI03DYXp",
        "outputId": "f2f0250f-755f-40f6-da74-298e90b03b4c"
      },
      "outputs": [],
      "source": [
        "#@title Test the Q&A Agent with sample questions\n",
        "\n",
        "test_questions = [\n",
        "    \"What is the main topic of this video?\",\n",
        "    \"What are the key points discussed?\",\n",
        "    \"Can you summarize what was said?\"\n",
        "]\n",
        "\n",
        "print(\"üß™ Testing Q&A Agent with sample questions:\\n\")\n",
        "\n",
        "for question in test_questions:\n",
        "    result = qa_agent.answer_question(question)\n",
        "\n",
        "    if result:\n",
        "        print(f\"\\nüí¨ ANSWER:\")\n",
        "        print(\"-\" * 60)\n",
        "        print(result['answer'])\n",
        "        print(\"-\" * 60)\n",
        "        print()\n",
        "\n",
        "    # Small delay between questions\n",
        "    import time\n",
        "    time.sleep(2)\n",
        "\n",
        "print(\"\\n‚úÖ Q&A Agent testing complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24vZGLSZEByx",
        "outputId": "fa803060-71fa-42be-cf7b-c7593d94a36b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Frame Extractor initialized\n",
            "\n",
            "üéØ Frame Extractor ready to use!\n"
          ]
        }
      ],
      "source": [
        "#@title Frame Extraction\n",
        "import cv2\n",
        "from moviepy.editor import VideoFileClip\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image, display\n",
        "import os\n",
        "import base64\n",
        "from PIL import Image as PILImage\n",
        "\n",
        "class FrameExtractor:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize Frame Extractor\"\"\"\n",
        "        print(\"‚úÖ Frame Extractor initialized\")\n",
        "\n",
        "    def extract_key_frames(self, video_path, num_frames=5):\n",
        "        \"\"\"Extract key frames from video at regular intervals\"\"\"\n",
        "        try:\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(\"üé¨ FRAME EXTRACTION STARTED\")\n",
        "            print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "            print(f\"üìπ Processing video: {video_path}\")\n",
        "\n",
        "            # Get video info\n",
        "            video = VideoFileClip(video_path)\n",
        "            duration = video.duration\n",
        "            fps = video.fps\n",
        "            total_frames = int(duration * fps)\n",
        "\n",
        "            print(f\"   Duration: {duration:.1f} seconds\")\n",
        "            print(f\"   FPS: {fps}\")\n",
        "            print(f\"   Total frames: {total_frames}\\n\")\n",
        "\n",
        "            # Calculate frame intervals\n",
        "            interval = duration / (num_frames + 1)\n",
        "            timestamps = [interval * (i + 1) for i in range(num_frames)]\n",
        "\n",
        "            print(f\"üéûÔ∏è  Extracting {num_frames} key frames...\")\n",
        "\n",
        "            # Create frames directory\n",
        "            os.makedirs('extracted_frames', exist_ok=True)\n",
        "\n",
        "            frames_data = []\n",
        "\n",
        "            for idx, timestamp in enumerate(timestamps):\n",
        "                # Extract frame at timestamp\n",
        "                frame = video.get_frame(timestamp)\n",
        "\n",
        "                # Save frame\n",
        "                frame_path = f'extracted_frames/frame_{idx+1}_at_{timestamp:.1f}s.jpg'\n",
        "                plt.imsave(frame_path, frame)\n",
        "\n",
        "                frames_data.append({\n",
        "                    'frame_number': idx + 1,\n",
        "                    'timestamp': timestamp,\n",
        "                    'path': frame_path\n",
        "                })\n",
        "\n",
        "                print(f\"   ‚úì Frame {idx+1} at {timestamp:.1f}s\")\n",
        "\n",
        "            video.close()\n",
        "\n",
        "            print(f\"\\n‚úÖ Extracted {len(frames_data)} frames\")\n",
        "            print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "            return frames_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Frame extraction error: {e}\")\n",
        "            return None\n",
        "\n",
        "    def display_frames(self, frames_data):\n",
        "        \"\"\"Display extracted frames\"\"\"\n",
        "        print(\"üñºÔ∏è  EXTRACTED FRAMES:\\n\")\n",
        "\n",
        "        for frame_info in frames_data:\n",
        "            print(f\"Frame {frame_info['frame_number']} (at {frame_info['timestamp']:.1f}s):\")\n",
        "            display(Image(filename=frame_info['path'], width=400))\n",
        "            print()\n",
        "\n",
        "# Initialize Frame Extractor\n",
        "frame_extractor = FrameExtractor()\n",
        "print(\"\\nüéØ Frame Extractor ready to use!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "E7wbUT1cXBBe",
        "outputId": "7bba547e-962a-4865-8f71-90681c302da9"
      },
      "outputs": [],
      "source": [
        "#@title Complete Gradio Interface\n",
        "import gradio as gr\n",
        "import base64\n",
        "from PIL import Image\n",
        "import tempfile\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "class VideoContentAnalyzer:\n",
        "    def __init__(self, transcription_agent, summary_agent, embedding_agent, qa_agent, frame_extractor):\n",
        "        self.transcription_agent = transcription_agent\n",
        "        self.summary_agent = summary_agent\n",
        "        self.embedding_agent = embedding_agent\n",
        "        self.qa_agent = qa_agent\n",
        "        self.frame_extractor = frame_extractor\n",
        "        self.current_transcript = None\n",
        "        self.current_frames = None\n",
        "        self.current_summary = None\n",
        "\n",
        "    def image_to_base64(self, image_path):\n",
        "        \"\"\"Convert image to base64 for HTML display\"\"\"\n",
        "        try:\n",
        "            with open(image_path, \"rb\") as img_file:\n",
        "                return base64.b64encode(img_file.read()).decode('utf-8')\n",
        "        except Exception as e:\n",
        "            print(f\"Error converting image to base64: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def create_text_file(self, content, filename):\n",
        "        \"\"\"Create a temporary text file for download\"\"\"\n",
        "        temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8')\n",
        "        temp_file.write(content)\n",
        "        temp_file.close()\n",
        "        return temp_file.name\n",
        "\n",
        "    def create_frames_zip(self, frames_data, zip_filename=\"frames.zip\"):\n",
        "        \"\"\"Create a ZIP file containing all extracted frames\"\"\"\n",
        "        try:\n",
        "            temp_zip = tempfile.NamedTemporaryFile(suffix='.zip', delete=False)\n",
        "            temp_zip.close()\n",
        "\n",
        "            with zipfile.ZipFile(temp_zip.name, 'w') as zipf:\n",
        "                for frame in frames_data:\n",
        "                    if os.path.exists(frame['path']):\n",
        "                        # Use descriptive filename in the zip\n",
        "                        frame_filename = f\"frame_{frame['frame_number']}_at_{frame['timestamp']:.1f}s.jpg\"\n",
        "                        zipf.write(frame['path'], frame_filename)\n",
        "\n",
        "            return temp_zip.name\n",
        "        except Exception as e:\n",
        "            print(f\"Error creating frames ZIP: {e}\")\n",
        "            return None\n",
        "\n",
        "    def process_video(self, video_file):\n",
        "        \"\"\"Process uploaded video - all agents work together\"\"\"\n",
        "        if video_file is None:\n",
        "            return \"Please upload a video file!\", \"\", \"\", \"\", gr.DownloadButton(visible=False), gr.DownloadButton(visible=False), gr.DownloadButton(visible=False)\n",
        "\n",
        "        try:\n",
        "            # Initialize download files as None\n",
        "            transcript_file = None\n",
        "            summary_file = None\n",
        "            frames_zip = None\n",
        "\n",
        "            # Step 1: Transcription\n",
        "            status = \"üé§ Transcribing video...\\n\"\n",
        "            yield status, \"\", \"\", \"\", gr.DownloadButton(visible=False), gr.DownloadButton(visible=False), gr.DownloadButton(visible=False)\n",
        "\n",
        "            transcript_result = self.transcription_agent.process_video(video_file)\n",
        "            if not transcript_result:\n",
        "                yield \"‚ùå Transcription failed!\", \"\", \"\", \"\", gr.DownloadButton(visible=False), gr.DownloadButton(visible=False), gr.DownloadButton(visible=False)\n",
        "                return\n",
        "\n",
        "            self.current_transcript = transcript_result['text']\n",
        "            status += f\"‚úÖ Transcription complete! ({len(transcript_result['words'])} words)\\n\\n\"\n",
        "\n",
        "            # Step 2: Summary\n",
        "            status += \"üìù Generating summary...\\n\"\n",
        "            yield status, self.current_transcript[:500] + \"...\", \"\", \"\", gr.DownloadButton(visible=False), gr.DownloadButton(visible=False), gr.DownloadButton(visible=False)\n",
        "\n",
        "            summary_result = self.summary_agent.process_transcript(self.current_transcript)\n",
        "            summary_text = f\"{summary_result['summary']}\\n\\n---\\n\\n{summary_result['chapters']}\"\n",
        "            self.current_summary = summary_text\n",
        "            status += \"‚úÖ Summary generated!\\n\\n\"\n",
        "\n",
        "            # Step 3: Embeddings\n",
        "            status += \"üß† Creating vector database...\\n\"\n",
        "            yield status, self.current_transcript[:500] + \"...\", summary_text, \"\", gr.DownloadButton(visible=False), gr.DownloadButton(visible=False), gr.DownloadButton(visible=False)\n",
        "\n",
        "            embedding_result = self.embedding_agent.store_embeddings(self.current_transcript)\n",
        "            status += f\"‚úÖ Vector DB ready! ({embedding_result['total_chunks']} chunks)\\n\\n\"\n",
        "\n",
        "            # Step 4: Frame Extraction\n",
        "            status += \"üé¨ Extracting key frames...\\n\"\n",
        "            yield status, self.current_transcript[:500] + \"...\", summary_text, \"\", gr.DownloadButton(visible=False), gr.DownloadButton(visible=False), gr.DownloadButton(visible=False)\n",
        "\n",
        "            self.current_frames = self.frame_extractor.extract_key_frames(video_file, num_frames=5)\n",
        "\n",
        "            # Create frames display with base64 images\n",
        "            frames_html = \"<div style='display: grid; grid-template-columns: repeat(2, 1fr); gap: 15px; padding: 10px;'>\"\n",
        "            if self.current_frames:\n",
        "                for frame in self.current_frames:\n",
        "                    base64_image = self.image_to_base64(frame['path'])\n",
        "                    if base64_image:\n",
        "                        frames_html += f\"\"\"\n",
        "                        <div style='text-align: center; border: 1px solid #ddd; border-radius: 10px; padding: 10px; background: #f9f9f9;'>\n",
        "                            <img src='data:image/jpeg;base64,{base64_image}' style='width: 100%; max-width: 300px; border-radius: 8px;'>\n",
        "                            <p style='margin: 8px 0; font-weight: bold;'>Frame {frame['frame_number']}</p>\n",
        "                            <p style='margin: 0; color: #666;'>Timestamp: {frame['timestamp']:.1f}s</p>\n",
        "                        </div>\n",
        "                        \"\"\"\n",
        "                    else:\n",
        "                        frames_html += f\"\"\"\n",
        "                        <div style='text-align: center; border: 1px solid #ddd; border-radius: 10px; padding: 10px; background: #f9f9f9;'>\n",
        "                            <p style='color: red;'>Image not found</p>\n",
        "                            <p>Frame {frame['frame_number']} @ {frame['timestamp']:.1f}s</p>\n",
        "                        </div>\n",
        "                        \"\"\"\n",
        "            else:\n",
        "                frames_html += \"<p>No frames extracted</p>\"\n",
        "\n",
        "            frames_html += \"</div>\"\n",
        "\n",
        "            # Create downloadable files\n",
        "            transcript_file = self.create_text_file(self.current_transcript, \"transcript.txt\")\n",
        "            summary_file = self.create_text_file(self.current_summary, \"summary.txt\")\n",
        "            frames_zip = self.create_frames_zip(self.current_frames) if self.current_frames else None\n",
        "\n",
        "            status += \"‚úÖ Frame extraction complete!\\n\\n\"\n",
        "            status += \"üéâ All processing complete! You can now ask questions about the video.\"\n",
        "\n",
        "            # Return results with download buttons\n",
        "            yield (\n",
        "                status,\n",
        "                self.current_transcript,\n",
        "                summary_text,\n",
        "                frames_html,\n",
        "                gr.DownloadButton(visible=True, value=transcript_file, label=\"üì• Download Transcript\"),\n",
        "                gr.DownloadButton(visible=True, value=summary_file, label=\"üì• Download Summary\"),\n",
        "                gr.DownloadButton(visible=True, value=frames_zip, label=\"üì• Download Frames\") if frames_zip else gr.DownloadButton(visible=False)\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            yield f\"‚ùå Error: {str(e)}\", \"\", \"\", \"\", gr.DownloadButton(visible=False), gr.DownloadButton(visible=False), gr.DownloadButton(visible=False)\n",
        "\n",
        "    def answer_question(self, question):\n",
        "        \"\"\"Answer question about the video\"\"\"\n",
        "        if not self.current_transcript:\n",
        "            return \"Please process a video first!\"\n",
        "\n",
        "        if not question or question.strip() == \"\":\n",
        "            return \"Please enter a question!\"\n",
        "\n",
        "        result = self.qa_agent.answer_question(question)\n",
        "\n",
        "        if result:\n",
        "            answer = f\"**Question:** {result['question']}\\n\\n\"\n",
        "            answer += f\"**Answer:** {result['answer']}\\n\\n\"\n",
        "            answer += \"---\\n\\n**Relevant Context:**\\n\\n\"\n",
        "            for i, source in enumerate(result['sources'][:2], 1):\n",
        "                answer += f\"{i}. {source[:200]}...\\n\\n\"\n",
        "            return answer\n",
        "        else:\n",
        "            return \"Sorry, I couldn't generate an answer.\"\n",
        "\n",
        "# Initialize the complete system\n",
        "analyzer = VideoContentAnalyzer(\n",
        "    transcription_agent,\n",
        "    summary_agent,\n",
        "    embedding_agent,\n",
        "    qa_agent,\n",
        "    frame_extractor\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Video Content Analyzer System Ready!\")\n",
        "\n",
        "# Create Gradio Interface\n",
        "with gr.Blocks(title=\"Video Content Analyzer - Agentic AI\", theme=gr.themes.Soft()) as demo:\n",
        "\n",
        "    gr.Markdown(\"\"\"\n",
        "    # üé• Video Content Analyzer - Agentic AI System\n",
        "\n",
        "    Upload a video and let multiple AI agents analyze it for you!\n",
        "\n",
        "    **Features:**\n",
        "    - üé§ Automatic transcription\n",
        "    - üìù Intelligent summarization\n",
        "    - üß† Semantic search with vector database\n",
        "    - üí¨ Q&A about video content\n",
        "    - üé¨ Key frame extraction\n",
        "    - üì• Download all results\n",
        "    \"\"\")\n",
        "\n",
        "    with gr.Tab(\"üì§ Upload & Process Video\"):\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                video_input = gr.Video(label=\"Upload Video File\")\n",
        "                process_btn = gr.Button(\"üöÄ Process Video\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "            with gr.Column():\n",
        "                status_output = gr.Textbox(\n",
        "                    label=\"Processing Status\",\n",
        "                    lines=10,\n",
        "                    max_lines=15\n",
        "                )\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                transcript_output = gr.Textbox(\n",
        "                    label=\"üìù Transcript\",\n",
        "                    lines=10,\n",
        "                    max_lines=20\n",
        "                )\n",
        "                download_transcript = gr.DownloadButton(\n",
        "                    \"üì• Download Transcript\",\n",
        "                    visible=False,\n",
        "                    variant=\"secondary\"\n",
        "                )\n",
        "\n",
        "            with gr.Column():\n",
        "                summary_output = gr.Textbox(\n",
        "                    label=\"üìä Summary & Chapters\",\n",
        "                    lines=10,\n",
        "                    max_lines=20\n",
        "                )\n",
        "                download_summary = gr.DownloadButton(\n",
        "                    \"üì• Download Summary\",\n",
        "                    visible=False,\n",
        "                    variant=\"secondary\"\n",
        "                )\n",
        "\n",
        "        frames_output = gr.HTML(label=\"üé¨ Extracted Key Frames\")\n",
        "\n",
        "        with gr.Row():\n",
        "            download_frames = gr.DownloadButton(\n",
        "                \"üì• Download All Frames (ZIP)\",\n",
        "                visible=False,\n",
        "                variant=\"secondary\"\n",
        "            )\n",
        "\n",
        "    with gr.Tab(\"üí¨ Ask Questions\"):\n",
        "        gr.Markdown(\"### Ask questions about the video content\")\n",
        "\n",
        "        with gr.Row():\n",
        "            question_input = gr.Textbox(\n",
        "                label=\"Your Question\",\n",
        "                placeholder=\"e.g., What is the main topic discussed?\",\n",
        "                lines=2\n",
        "            )\n",
        "\n",
        "        ask_btn = gr.Button(\"üîç Get Answer\", variant=\"primary\")\n",
        "\n",
        "        answer_output = gr.Markdown(label=\"Answer\")\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        **Example Questions:**\n",
        "        - What is the main topic of this video?\n",
        "        - What are the key points discussed?\n",
        "        - Can you explain [specific topic] mentioned in the video?\n",
        "        - What conclusions were drawn?\n",
        "        \"\"\")\n",
        "\n",
        "    # Event handlers\n",
        "    process_btn.click(\n",
        "        fn=analyzer.process_video,\n",
        "        inputs=[video_input],\n",
        "        outputs=[\n",
        "            status_output,\n",
        "            transcript_output,\n",
        "            summary_output,\n",
        "            frames_output,\n",
        "            download_transcript,\n",
        "            download_summary,\n",
        "            download_frames\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    ask_btn.click(\n",
        "        fn=analyzer.answer_question,\n",
        "        inputs=[question_input],\n",
        "        outputs=[answer_output]\n",
        "    )\n",
        "\n",
        "# Launch the interface\n",
        "print(\"üöÄ Launching Gradio Interface...\")\n",
        "try:\n",
        "    demo.launch(share=True, debug=True)\n",
        "except Exception as e:\n",
        "    print(f\"Note: If you encounter issues with share=True, try running with share=False\")\n",
        "    demo.launch(share=False, debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
